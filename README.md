# üìä An√°lise de Churn TelecomX Parte 2

# Tecnologias Utilizadas

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![Pandas](https://img.shields.io/badge/Pandas-1.3+-green.svg)
![NumPy](https://img.shields.io/badge/NumPy-1.21+-013243.svg)
![Scikit--Learn](https://img.shields.io/badge/Scikit--Learn-1.0+-F7931E.svg)
![Matplotlib](https://img.shields.io/badge/Matplotlib-3.5+-orange.svg)
![Seaborn](https://img.shields.io/badge/Seaborn-0.11+-red.svg)
![Statsmodels](https://img.shields.io/badge/Statsmodels-0.13+-purple.svg)
![Imbalanced--Learn](https://img.shields.io/badge/Imbalanced--Learn-0.8+-brightgreen.svg)
![XGBoost](https://img.shields.io/badge/XGBoost-1.6+-darkgreen.svg)

## T√©cnicas e Algoritmos

![SMOTE](https://img.shields.io/badge/Balanceamento-SMOTE-yellow.svg)
![StandardScaler](https://img.shields.io/badge/Normaliza√ß√£o-StandardScaler-lightblue.svg)
![Logistic Regression](https://img.shields.io/badge/Modelo-Logistic%20Regression-blueviolet.svg)
![XGBoost Model](https://img.shields.io/badge/Modelo-XGBoost-success.svg)

## üéØ Objetivos do Projeto
- Preparar os dados para a modelagem (tratamento, encoding, normaliza√ß√£o).
- Realizar an√°lise de correla√ß√£o e sele√ß√£o de vari√°veis.
- Treinar dois ou mais modelos de classifica√ß√£o.
- Avaliar o desempenho dos modelos com m√©tricas.
- Interpretar os resultados, incluindo a import√¢ncia das vari√°veis.
- Criar uma conclus√£o estrat√©gica apontando os principais fatores que influenciam a evas√£o.

## üöÄ Como executar o projeto

1. Fa√ßa o download dos arquivos deste reposit√≥rio:
   - `Challenge_TelecomX_Parte_2_Lidia_Lapertosa.ipynb`
   - `df_limpo.csv` (arquivo de dados)

2. Abra o **Google Colab** ([link](https://colab.research.google.com/)).

3. Fa√ßa o upload dos arquivos para o Colab.

4. No notebook, importe o arquivo CSV utilizando **Pandas**:

   ```python
   import pandas as pd
   df = pd.read_csv("df_limpo.csv")

5. Execute as c√©lulas na ordem em que est√£o no notebook para reproduzir toda a an√°lise.

## üìù Metodologia

O projeto foi desenvolvido em diferentes etapas:

1. **Prepara√ß√£o dos Dados**
   * Tratamento de valores ausentes
   * Padroniza√ß√£o de vari√°veis categ√≥ricas e num√©ricas

2. **Correla√ß√£o e Sele√ß√£o de Vari√°veis**
   * An√°lise explorat√≥ria
   * Identifica√ß√£o das vari√°veis mais relevantes para o churn

3. **Modelagem Preditiva**
   * Treinamento com **Logistic Regression** e **XGBoost**

4. **An√°lise dos Modelos**
   * Avalia√ß√£o usando m√©tricas de classifica√ß√£o: AUC, F1-Score, Precision e Recall

5. **Import√¢ncia das Vari√°veis**
   * An√°lise da contribui√ß√£o de cada vari√°vel em cada modelo
   * Visualiza√ß√£o gr√°fica da import√¢ncia das features

6. **Conclus√£o**
   * Principais insights
   * Recomenda√ß√£o final para a empresa

## ü§ñ Modelos Utilizados

Foram selecionados dois algoritmos complementares para an√°lise comparativa:

**Logistic Regression**

* Modelo linear probabil√≠stico com alta interpretabilidade
* Coeficientes fornecem insights diretos sobre impacto das features
* Baseline robusto para problemas de classifica√ß√£o bin√°ria

**XGBoost (Extreme Gradient Boosting)**

* Ensemble de √°rvores de decis√£o com boosting
* Captura intera√ß√µes complexas e rela√ß√µes n√£o-lineares
* Otimiza√ß√£o autom√°tica de hiperpar√¢metros e regulariza√ß√£o

## üîç Compara√ß√£o de M√©tricas

| M√©trica | Logistic Regression | XGBoost | ü•á Vencedor |
|---------|-------------------|---------|-------------|
| **Acur√°cia** | 75.1% | 76.5% | XGBoost |
| **ROC AUC** | 84.5% | 80.9% | **Logistic Regression** |
| **Recall (Churn)** | 81% | 56% | **Logistic Regression** |
| **Precision (Churn)** | 52% | 56% | XGBoost |

## üìä Principais Vari√°veis

Os modelos identificaram vari√°veis com maior impacto no churn.

* **Logistic Regression**

<img width="821" height="590" alt="variaveis_regressao_logistica" src="https://github.com/user-attachments/assets/76fd9119-6d62-4c7a-bd1c-be8d5428c7e6" />

* **XGBoost**
<img width="790" height="590" alt="variaveis_xgboost" src="https://github.com/user-attachments/assets/f9d28101-6625-4da6-8960-a2b1e8c9771e" />

## üìå Conclus√£o e Recomenda√ß√µes

* Clientes com **menor tempo de contrato** apresentam maior probabilidade de churn.
* O **tipo de servi√ßo de internet (Fiber Optic)** foi uma vari√°vel fortemente associada ao cancelamento.
* **Contratos mais longos (2 anos)** reduzem significativamente o churn.

**Recomenda√ß√£o Final:**
A empresa deve investir em **programas de fideliza√ß√£o** e **melhoria na percep√ß√£o do servi√ßo de internet fibra**, j√° que esses fatores s√£o cruciais para reten√ß√£o.

## üí° Principais Insights

* O **XGBoost** apresentou maior acur√°cia geral, mas o **Logistic Regression** se destacou em Recall (essencial em churn, pois minimizar falsos negativos √© mais importante).
* Vari√°veis de **contrato e tempo de perman√™ncia** s√£o determinantes para prever churn.
* H√° espa√ßo para otimizar campanhas de reten√ß√£o segmentadas por perfil de cliente.

## Pr√≥ximos passos
1. Pr√©-processamento dos dados
- **Testar outras t√©cnicas de balanceamento al√©m do SMOTE**:
  - **SMOTEENN** ou **SMOTETomek** ‚Üí combina oversampling + limpeza de ru√≠do.
  - **Random Undersampling** ‚Üí remover clientes n√£o churn em excesso.
  - **Class Weights** ‚Üí penalizar mais os erros da classe churn sem alterar os dados.
    ```python
    LogisticRegression(class_weight='balanced')
    XGBClassifier(scale_pos_weight=ratio)
    ```
- **Feature Engineering**  
  Criar novas vari√°veis pode ter mais impacto que trocar de modelo:
  - Tempo m√©dio at√© cancelamento.
  - Raz√£o entre gasto atual e gasto hist√≥rico.
  - N√∫mero de intera√ß√µes recentes do cliente.

2. Modelagem
- **Valida√ß√£o cruzada (Cross-Validation, ex. StratifiedKFold)**  
  Evita depender de uma √∫nica divis√£o treino/teste e gera resultados mais confi√°veis.

- **Ajuste de hiperpar√¢metros (Hyperparameter Tuning)**  
  Tanto Logistic Regression quanto XGBoost podem melhorar bastante com `GridSearchCV`, `RandomizedSearchCV` ou **Optuna**.
  - Logistic Regression ‚Üí par√¢metro `C` (regulariza√ß√£o), solver.  
  - XGBoost ‚Üí par√¢metros como `n_estimators`, `max_depth`, `learning_rate`, `subsample`, `colsample_bytree`.

- **Ensembles**  
  Combinar modelos pode melhorar a performance:
  - Stacking ou blending de Logistic Regression + XGBoost.  
  - Une recall alto de um com precis√£o do outro.

3. Avalia√ß√£o
- **M√©tricas focadas no neg√≥cio**  
  Como churn √© desbalanceado, acur√°cia pode enganar.  
  - Usar **F1-score** (equil√≠brio entre recall e precis√£o).  
  - Usar **PR AUC (Precision-Recall AUC)** ‚Üí mais informativo para dados desbalanceados.  

- **Threshold Tuning**  
  Atualmente, o corte padr√£o `0.5` est√° sendo usado.  
  - Ajustar o threshold pode aumentar recall sem perder tanto na precis√£o.  
  - Exemplo: usar `predict_proba` e escolher o ponto √≥timo na curva Precision-Recall.
 
## üìû Contato

<div align="left">
<img src="https://github.com/user-attachments/assets/07594e84-2524-4810-a5dc-58abc9526ca3" alt="imagem autora do projeto" width="150px">

**Lidia Lapertosa**

[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/lidia-lapertosa/)

üìß Em caso de d√∫vidas, entre em contato!
